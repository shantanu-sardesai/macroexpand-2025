(cpp/raw "#include <iostream>")
(cpp/raw "#include <torch/torch.h>")

;; Define a new Module.
(cpp/raw "
struct Net : torch::nn::Module {
  Net() {
    // Construct and register two Linear submodules.
    fc1 = register_module(\"fc1\", torch::nn::Linear(784, 64));
    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));
    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));
  }

  // Implement the Net's algorithm.
  torch::Tensor forward(torch::Tensor x) {
    // Use one of many tensor manipulation functions.
    x = torch::relu(fc1->forward(x.reshape({x.size(0), 784})));
    x = torch::dropout(x, /*p=*/0.5, /*train=*/is_training());
    x = torch::relu(fc2->forward(x));
    x = torch::log_softmax(fc3->forward(x), /*dim=*/1);
    return x;
  }

  // Use one of many \"standard library\" modules.
  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};
};
")

(comment
  :info
  "
  This is required because the overload resolution for this call in jank C++ interop didn't work.
  "
  '(cpp/.map mnist stack)  ; Placing this in place of the function call form should replicate the issue.

  :fixme
  "
  ─ analyze/invalid-cpp-call ─────────────────────────────────────────────────────────────────────────
    error: No matching call to 'map' function. With argument 0 having type 'torch::data::datasets::MNIST
           &'. With argument 1 having type 'torch::data::transforms::Stack<torch::data::Example<> > &'.
  ")
(cpp/raw "
inline decltype(auto) map_data(torch::data::datasets::MNIST mnist, torch::data::transforms::Stack<torch::data::Example<>> stack)
{
  return mnist.map(stack);
}
")

(comment
  :info
  '(cpp/torch.nll_loss prediction (cpp/.-target batch))  ; Placing this in place of the function call form should replicate the issue.

  :fixme
  "
  ─ analyze/unresolved-cpp-symbol ────────────────────────────────────────────────────────────────────
    error: Unable to find 'nll_loss' within namespace 'torch'.
  ")
(cpp/raw "
inline decltype(auto) call_nll_loss(auto prediction, auto batch_target) {
  return torch::nll_loss(prediction, batch_target);
}
")

(comment
  :info
  "Since currently jank doesn't support passing in an explicit type parameter."
  )
(cpp/raw "
inline decltype(auto) get_item(auto loss) {
  return loss.template item<float>();
}
")

(comment
  :info
  "https://github.com/pytorch/pytorch/blob/908c5cc4c0f22d141776bde47c296b5186691855/torch/csrc/api/include/torch/data/dataloader.h#L31"
  '(cpp/torch.data.make_data_loader dataset (cpp/size_t 64))  ; Placing this in place of the function call form should replicate the issue.

  :fixme
  "CppInterOp codegen bug."
  )
(cpp/raw "
inline decltype(auto) make_data_loader(torch::data::datasets::MapDataset<torch::data::datasets::MNIST, torch::data::transforms::Stack<torch::data::Example<>>> dataset, std::size_t batch_size) {
  return torch::data::make_data_loader(dataset, batch_size);
}
")

(cpp/raw "using TorchStack = torch::data::transforms::Stack<torch::data::Example<>>;")

(defn train []
  (let [net (cpp/value "std::make_shared<Net>()")
        mnist (cpp/torch.data.datasets.MNIST (cpp/cast cpp/std.string "./data"))
        stack ((cpp/type "TorchStack"))
        dataset (cpp/map_data mnist stack)
        data-loader (cpp/make_data_loader dataset (cpp/size_t 64)) ;; Create a multi-threaded data loader for the MNIST dataset.
        optimizer (-> net cpp/* cpp/.parameters (cpp/torch.optim.SGD (cpp/double 0.01))) ;; Instantiate an SGD optimization algorithm to update our Net's parameters.
        newline (cpp/cast cpp/std.string "\n")]
    (dotimes [epoch 1]
      (let [it (-> data-loader cpp/* cpp/.begin)
            end (-> data-loader cpp/* cpp/.end)]
        (loop [batch-index 0]
          ; INFO: Operator re-write by the C++ compiler: https://github.com/jank-lang/jank/issues/413
          (when (cpp/! (cpp/== it end))
            (cpp/.zero_grad optimizer)
            (let [batch (cpp/* it)
                  prediction (cpp/.forward (cpp/* net) (cpp/.-data batch))
                  loss (cpp/call_nll_loss prediction (cpp/.-target batch))]
              (cpp/.backward loss)
              (cpp/.step optimizer)
              (when (-> batch-index (rem 100) zero?)
                (cpp/<< cpp/std.cout (cpp/cast cpp/std.string (str
                                                                "Epoch: " epoch
                                                                " | Batch: " batch-index
                                                                " | Loss: " (cpp/get_item loss) 
                                                                \newline)))
                (cpp/torch.save net (cpp/cast cpp/std.string "net.pt")))
                (cpp/++ it)
                (recur (inc batch-index)))))))
    (cpp/<< cpp/std.cout (cpp/cast cpp/std.string "Training complete"))
    (cpp/<< cpp/std.cout newline)
    nil))

(train)

