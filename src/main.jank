(cpp/raw "#include <iostream>")
(cpp/raw "#include <torch/torch.h>")

;; Define a new Module.
(cpp/raw "
struct Net : torch::nn::Module {
  Net() {
    // Construct and register two Linear submodules.
    fc1 = register_module(\"fc1\", torch::nn::Linear(784, 64));
    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));
    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));
  }

  // Implement the Net's algorithm.
  torch::Tensor forward(torch::Tensor x) {
    // Use one of many tensor manipulation functions.
    x = torch::relu(fc1->forward(x.reshape({x.size(0), 784})));
    x = torch::dropout(x, /*p=*/0.5, /*train=*/is_training());
    x = torch::relu(fc2->forward(x));
    x = torch::log_softmax(fc3->forward(x), /*dim=*/1);
    return x;
  }

  // Use one of many \"standard library\" modules.
  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};
};
")

; This is required because the overload resolution for this call in jank C++ interop didn't work.
; The return type is following this slack thread: https://clojurians.slack.com/archives/C03SRH97FDK/p1757187767548989?thread_ts=1757169396.120749&cid=C03SRH97FDK
(cpp/raw "
inline decltype(auto) map_data(torch::data::datasets::MNIST mnist, torch::data::transforms::Stack<torch::data::Example<>> stack)
{
  return mnist.map(stack);
}
")
(cpp/raw "
inline decltype(auto) call_nll_loss(auto prediction, auto batch_target) {
  return torch::nll_loss(prediction, batch_target);
}
")

; A very interesting reason as to why it's required, read more here: https://github.com/jank-lang/jank/issues/413.
(cpp/raw "bool it_neq(auto lhs, auto rhs) { return lhs != rhs; } ")

; Since currently jank doesn't support passing in an explicit type parameter.
(cpp/raw "
inline decltype(auto) get_item(auto loss) {
  // return loss.item<float>();
  return 100;
}")

; https://github.com/pytorch/pytorch/blob/908c5cc4c0f22d141776bde47c296b5186691855/torch/csrc/api/include/torch/data/dataloader.h#L31
(cpp/raw "
inline decltype(auto) map_data_loader(torch::data::datasets::MapDataset<torch::data::datasets::MNIST, torch::data::transforms::Stack<torch::data::Example<>>> dataset, int batch_size) {
  return torch::data::make_data_loader(dataset, batch_size);
}
")

(defn train []
  (let [net (cpp/box (cpp/& (cpp/value "std::make_shared<Net>()")))
        mnist (cpp/torch.data.datasets.MNIST (cpp/cast cpp/std.string "./data"))
        stack ((cpp/type "torch::data::transforms::Stack<torch::data::Example<>>"))
        dataset (cpp/map_data mnist stack)
        data-loader (cpp/box (cpp/& (cpp/torch.data.make_data_loader dataset (cpp/size_t 64)))) ;; Create a multi-threaded data loader for the MNIST dataset.
        optimizer (cpp/box (cpp/& (cpp/torch.optim.SGD (cpp/.parameters (cpp/* (cpp/* (cpp/unbox (cpp/type "std::shared_ptr<Net>*") net)))) (cpp/float 0.01)))) ;; Instantiate an SGD optimization algorithm to update our Net's parameters.
        newline (cpp/cast cpp/std.string "\n")]
    (loop [epoch 1]
      (when (<= epoch 1)
        (let [batch-index 0
              it (cpp/box (cpp/& (cpp/.begin (cpp/.get (cpp/unbox (cpp/type "std::enable_if_t<!torch::data::datasets::MapDataset<torch::data::datasets::MNIST, torch::data::transforms::Stack<torch::data::Example<>>>::is_stateful && std::is_constructible_v<torch::data::samplers::RandomSampler, size_t>, std::unique_ptr<torch::data::StatelessDataLoader<torch::data::datasets::MapDataset<torch::data::datasets::MNIST, torch::data::transforms::Stack<torch::data::Example<>>>, torch::data::samplers::RandomSampler>>>*") data-loader)))))
              end (cpp/box (cpp/& (cpp/.end (cpp/.get (cpp/unbox (cpp/type "std::enable_if_t<!torch::data::datasets::MapDataset<torch::data::datasets::MNIST, torch::data::transforms::Stack<torch::data::Example<>>>::is_stateful && std::is_constructible_v<torch::data::samplers::RandomSampler, size_t>, std::unique_ptr<torch::data::StatelessDataLoader<torch::data::datasets::MapDataset<torch::data::datasets::MNIST, torch::data::transforms::Stack<torch::data::Example<>>>, torch::data::samplers::RandomSampler>>>*") data-loader)))))]
          (loop []
            (when (cpp/it_neq (cpp/unbox (cpp/type "torch::data::Iterator<torch::data::Example<at::Tensor, at::Tensor>>*") it) (cpp/unbox (cpp/type "torch::data::Iterator<torch::data::Example<at::Tensor, at::Tensor>>*") end))
              (cpp/.zero_grad (cpp/unbox cpp/torch.optim.SGD* optimizer))
              (let [batch (cpp/* (cpp/* (cpp/unbox (cpp/type "torch::data::Iterator<torch::data::Example<at::Tensor, at::Tensor>>*") it)))
                    prediction (cpp/.forward (cpp/* (cpp/* (cpp/unbox (cpp/type "std::shared_ptr<Net>*") net))) (cpp/.-data batch))
                    loss (cpp/call_nll_loss prediction (cpp/.-target batch))] 
                (cpp/.backward loss)
                (cpp/.step (cpp/unbox cpp/torch.optim.SGD* optimizer))
                (if (zero? (rem (inc batch-index) 100))
                  (do
                    (cpp/<< cpp/std.cout (cpp/cast cpp/std.string (str "Epoch: " epoch
                                                                     " | Batch: " batch-index
                                                                     " | Loss: " (cpp/get_item loss) ; FIXME: No idea how to pass a explicit type parameter when calling a member function from jank land.
                                                                     \newline)))
                    (cpp/torch.save (cpp/* (cpp/unbox (cpp/type "std::shared_ptr<Net>*") net)) (cpp/cast cpp/std.string "net.pt"))
                    nil)))
              (cpp/++ it)
              (recur))))
        (recur (inc epoch))))
    (cpp/<< cpp/std.cout (cpp/cast cpp/std.string "Training complete"))
    (cpp/<< cpp/std.cout newline)
    nil)
  nil)

(train)

