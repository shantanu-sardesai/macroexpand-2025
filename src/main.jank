(cpp/raw "#include <iostream>")
(cpp/raw "#include <torch/torch.h>")

;; Define a new Module.
(cpp/raw "
struct Net : torch::nn::Module {
  Net() {
    // Construct and register two Linear submodules.
    fc1 = register_module(\"fc1\", torch::nn::Linear(784, 64));
    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));
    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));
  }

  // Implement the Net's algorithm.
  torch::Tensor forward(torch::Tensor x) {
    // Use one of many tensor manipulation functions.
    x = torch::relu(fc1->forward(x.reshape({x.size(0), 784})));
    x = torch::dropout(x, /*p=*/0.5, /*train=*/is_training());
    x = torch::relu(fc2->forward(x));
    x = torch::log_softmax(fc3->forward(x), /*dim=*/1);
    return x;
  }

  // Use one of many \"standard library\" modules.
  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};
};
")

(comment
  :info
  "
  This is required because the overload resolution for this call in jank C++ interop didn't work.
  "
  '(cpp/.map mnist stack)  ; Placing this in place of the function call form should replicate the issue.

  :fixme
  "
  ─ analyze/invalid-cpp-call ─────────────────────────────────────────────────────────────────────────
    error: No matching call to 'map' function. With argument 0 having type 'torch::data::datasets::MNIST
           &'. With argument 1 having type 'torch::data::transforms::Stack<torch::data::Example<> > &'.
  ")
(cpp/raw "
inline decltype(auto) map_data(torch::data::datasets::MNIST mnist, torch::data::transforms::Stack<torch::data::Example<>> stack)
{
  return mnist.map(stack);
}
")

(comment
  :info
  '(cpp/torch.nll_loss prediction (cpp/.-target batch))  ; Placing this in place of the function call form should replicate the issue.

  :fixme
  "
  ─ analyze/unresolved-cpp-symbol ────────────────────────────────────────────────────────────────────
    error: Unable to find 'nll_loss' within namespace 'torch'.
  ")
(cpp/raw "
inline decltype(auto) call_nll_loss(auto prediction, auto batch_target) {
  return torch::nll_loss(prediction, batch_target);
}
")

(comment
  :info
  "Since currently jank doesn't support passing in an explicit type parameter."
  )
(cpp/raw "
inline decltype(auto) get_item(auto loss) {
  return loss.template item<float>();
}
")

(comment
  :info
  "https://github.com/pytorch/pytorch/blob/908c5cc4c0f22d141776bde47c296b5186691855/torch/csrc/api/include/torch/data/dataloader.h#L31"
  '(cpp/torch.data.make_data_loader dataset (cpp/size_t 64))  ; Placing this in place of the function call form should replicate the issue.

  :fixme
  "CppInterOp codegen bug."
  )
(cpp/raw "
inline decltype(auto) make_data_loader(torch::data::datasets::MapDataset<torch::data::datasets::MNIST, torch::data::transforms::Stack<torch::data::Example<>>> dataset, std::size_t batch_size) {
  return torch::data::make_data_loader(dataset, batch_size);
}
")

(defn train []
  (let [net (cpp/value "std::make_shared<Net>()")
        mnist (cpp/torch.data.datasets.MNIST (cpp/cast cpp/std.string "./data"))
        stack ((cpp/type "torch::data::transforms::Stack<torch::data::Example<>>"))
        dataset (cpp/map_data mnist stack)
        data-loader (cpp/make_data_loader dataset (cpp/size_t 64)) ;; Create a multi-threaded data loader for the MNIST dataset.
        optimizer (-> (cpp/* net) cpp/.parameters (cpp/torch.optim.SGD (cpp/double 0.01)))
        newline (cpp/cast cpp/std.string "\n")
        ; INFO: We need to box these variables currently since loops are implemented in terms
        ;       of function recursion.
        net' (cpp/box (cpp/& net))
        data-loader' (cpp/box (cpp/& data-loader)) ;; Create a multi-threaded data loader for the MNIST dataset.
        optimizer' (cpp/box (cpp/& optimizer)) ;; Instantiate an SGD optimization algorithm to update our Net's parameters.
       ]
    (dotimes [epoch 1]
      (let [batch-index 0
            data-loader (cpp/unbox (cpp/type "std::enable_if_t<!torch::data::datasets::MapDataset<torch::data::datasets::MNIST, torch::data::transforms::Stack<torch::data::Example<>>>::is_stateful && std::is_constructible_v<torch::data::samplers::RandomSampler, size_t>, std::unique_ptr<torch::data::StatelessDataLoader<torch::data::datasets::MapDataset<torch::data::datasets::MNIST, torch::data::transforms::Stack<torch::data::Example<>>>, torch::data::samplers::RandomSampler>>>*") data-loader')
            it (-> data-loader cpp/.get cpp/.begin)
            end (-> data-loader cpp/.get cpp/.end)]
        ; INFO: Operator re-write by the C++ compiler: https://github.com/jank-lang/jank/issues/413
        (when (cpp/! (cpp/== it end))
          (cpp/.zero_grad (cpp/unbox cpp/torch.optim.SGD* optimizer'))
          (let [batch (cpp/* it)
                net (cpp/unbox (cpp/type "std::shared_ptr<Net>*") net')
                prediction (cpp/.forward (-> net cpp/* cpp/*) (cpp/.-data batch))
                loss (cpp/call_nll_loss prediction (cpp/.-target batch))
                optimizer (cpp/unbox cpp/torch.optim.SGD* optimizer')]
            (cpp/.backward loss)
            (cpp/.step optimizer)
            (if (-> batch-index inc (rem 100) zero?)
              (do
                (cpp/<< cpp/std.cout (cpp/cast cpp/std.string (str
                                                                "Epoch: " epoch
                                                                " | Batch: " batch-index
                                                                " | Loss: " (cpp/get_item loss) 
                                                                \newline)))
                (cpp/torch.save (cpp/* net) (cpp/cast cpp/std.string "net.pt")))))
          (cpp/++ it)
          nil)))
    (cpp/<< cpp/std.cout (cpp/cast cpp/std.string "Training complete"))
    (cpp/<< cpp/std.cout newline)
    nil))

(train)

