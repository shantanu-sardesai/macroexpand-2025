(cpp/raw "#include <iostream>")
(cpp/raw "#include <torch/torch.h>")

;; Define a new Module.
(cpp/raw "
struct Net : torch::nn::Module {
  Net() {
    // Construct and register two Linear submodules.
    fc1 = register_module(\"fc1\", torch::nn::Linear(784, 64));
    fc2 = register_module(\"fc2\", torch::nn::Linear(64, 32));
    fc3 = register_module(\"fc3\", torch::nn::Linear(32, 10));
  }

  // Implement the Net's algorithm.
  torch::Tensor forward(torch::Tensor x) {
    // Use one of many tensor manipulation functions.
    x = torch::relu(fc1->forward(x.reshape({x.size(0), 784})));
    x = torch::dropout(x, /*p=*/0.5, /*train=*/is_training());
    x = torch::relu(fc2->forward(x));
    x = torch::log_softmax(fc3->forward(x), /*dim=*/1);
    return x;
  }

  // Use one of many \"standard library\" modules.
  torch::nn::Linear fc1{nullptr}, fc2{nullptr}, fc3{nullptr};
};
")

; This is required because the overload resolution for this call in jank C++ interop didn't work.
(cpp/raw "
auto map_data(torch::data::datasets::MNIST mnist, torch::data::transforms::Stack<> stack)
{
  return mnist.map(stack);
}
")

; A very interesting reason as to why it's required, read more here: https://github.com/jank-lang/jank/issues/413.
(cpp/raw "bool it_neq(auto lhs, auto rhs) { return lhs != rhs; } ")

(defn train []
  (let [net (cpp/value "std::make_shared<Net>()")
        mnist (cpp/torch.data.datasets.MNIST (cpp/cast cpp/std.string "./data"))
        stack ((cpp/type "torch::data::transforms::Stack<>"))
        dataset (cpp/map_data mnist stack)
        data-loader (cpp/torch.data.make_data_loader dataset (cpp/int 64)) ;; Create a multi-threaded data loader for the MNIST dataset.
        optimizer (cpp/torch.optim.SGD (cpp/.parameters (cpp/* net)) (cpp/float 0.01)) ;; Instantiate an SGD optimization algorithm to update our Net's parameters.
        newline (cpp/cast cpp/std.string "\n")]
    (loop [epoch 1]
      (when (<= epoch 1)
        (let [batch-index 0
              it (cpp/.begin (cpp/* data-loader))
              end (cpp/.end (cpp/* data-loader))]
          (loop []
            (when (cpp/it_neq it end)
              (cpp/.zero_grad optimizer)
              (let [batch (cpp/* it)
                    prediction (cpp/.forward (cpp/* net) (cpp/.-data batch))
                    loss (cpp/torch.nll_loss prediction (cpp/.-target batch))] ; FIXME: Unable to find 'nll_loss' within namespace 'torch'.
                (cpp/.backward loss)
                (cpp/.step optimizer)
                (if (zero? (rem (inc batch-index) 100))
                  (cpp/<< cpp/std.cout (cpp/cast cpp/std.string (str "Epoch: " epoch
                                                                     " | Batch: " batch-index
                                                                     " | Loss: " (cpp/.item loss (cpp/type cpp/float)) ; FIXME: No idea how to pass a explicit type parameter when calling a member function from jank land.
                                                                     \newline)))
                  (cpp/torch.save net "net.pt")))
              (cpp/++ data-loader)
              (recur))))
      (recur (inc epoch))))
    (cpp/<< cpp/std.cout (cpp/cast cpp/std.string "Training complete"))
    (cpp/<< cpp/std.cout newline)))

(train)

